# -*- coding: utf-8 -*-
"""ChatBot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ti4FEHT2_3kqvnS2wjOEZOpPqIPozHpm
"""

!pip install pypdf

!pip install -q transformers accelerate langchain bitsandbytes

!pip install sentence_transformers

!pip install llama-index-llms-huggingface
!pip install llama-index-llms-huggingface-api

!pip install llama-index
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.core.prompts.prompts import SimpleInputPrompt

documents=SimpleDirectoryReader(input_dir='/content/jj').load_data()
documents

documents[0].metadata

total_pages = len(documents)
total_pages

system_prompt="""
You are a Q&A assistant. Your goal is to answer questions as accurately as possible based on the instructions and context provided.
"""

## Default format supportable by Llama2
query_wrapper_prompt=SimpleInputPrompt("<|USER|>{query_str}<|ASSISTANT|>")

!huggingface-cli login

import torch
llm = HuggingFaceLLM(
    context_window=4096,
    max_new_tokens=256,
    generate_kwargs={"temperature": 0.0, "do_sample": False},
    system_prompt=system_prompt,
    query_wrapper_prompt=query_wrapper_prompt,
    tokenizer_name="meta-llama/llama-2-7b-chat-hf",
    model_name="meta-llama/llama-2-7b-chat-hf",
    device_map="auto",
)

!pip install langchain_huggingface
!pip install llama-index-embeddings-langchain

from langchain_huggingface import HuggingFaceEmbeddings
from llama_index.embeddings.langchain import LangchainEmbedding

embed_model=LangchainEmbedding(
    HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2"))

from llama_index.core import Settings

Settings.embed_model = embed_model
Settings.llm = llm
Settings.chunk_size = 24

index = VectorStoreIndex.from_documents(documents)

index

query_engine=index.as_query_engine()

response = query_engine.query("what is the course code?")
response

response.response

app_code = """
import gradio as gr
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.core.prompts.prompts import SimpleInputPrompt
from llama_index.embeddings.langchain import LangchainEmbedding
from langchain_huggingface import HuggingFaceEmbeddings
import torch
from llama_index.core import Settings

# Load documents
documents = SimpleDirectoryReader(input_dir="data").load_data()

# Prompts
system_prompt = \"\"\"
You are a Q&A assistant. Your goal is to answer questions as accurately as possible based on the instructions and context provided.
\"\"\"
query_wrapper_prompt = SimpleInputPrompt("<|USER|>{query_str}<|ASSISTANT|>")

# Load LLaMA2 model
llm = HuggingFaceLLM(
    context_window=4096,
    max_new_tokens=256,
    generate_kwargs={"temperature": 0.0, "do_sample": False},
    system_prompt=system_prompt,
    query_wrapper_prompt=query_wrapper_prompt,
    tokenizer_name="meta-llama/llama-2-7b-chat-hf",
    model_name="meta-llama/llama-2-7b-chat-hf",
    device_map="auto"
)

# Embeddings
embed_model = LangchainEmbedding(
    HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")
)

# Settings
Settings.embed_model = embed_model
Settings.llm = llm
Settings.chunk_size = 24

# Indexing
index = VectorStoreIndex.from_documents(documents)
query_engine = index.as_query_engine()

# Gradio UI
def chat(query):
    return str(query_engine.query(query))

gr.Interface(fn=chat,
             inputs=gr.Textbox(placeholder="Ask a question..."),
             outputs="text",
             title="LLaMA2 Chatbot with RAG").launch()
"""

with open("app.py", "w") as f:
    f.write(app_code)

requirements = """
gradio
torch
transformers
accelerate
sentence-transformers
llama-index
llama-index-llms-huggingface
llama-index-llms-huggingface-api
llama-index-embeddings-langchain
langchain_huggingface
"""
with open("requirements.txt", "w") as f:
    f.write(requirements.strip())

!pip install -r requirements.txt

!pip install torch --upgrade
!pip install transformers accelerate sentence-transformers
!pip install llama-index llama-index-llms-huggingface llama-index-llms-huggingface-api
!pip install langchain_huggingface gradio pypdf

import gradio as gr
import os
import shutil
import torch

from huggingface_hub import login
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.core.prompts.prompts import SimpleInputPrompt
from langchain_huggingface import HuggingFaceEmbeddings
from llama_index.embeddings.langchain import LangchainEmbedding

# Optional: Login if you're using LLaMA2 gated model
login("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")

# ==== Use GPU if available ====
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# ==== Prompts and Model ====
system_prompt = """You are a Q&A assistant. Your goal is to answer questions as accurately as possible based on the context provided."""
query_wrapper_prompt = SimpleInputPrompt("<|USER|>{query_str}<|ASSISTANT|>")

# LLaMA2 model (or swap for another open-access one)
llm = HuggingFaceLLM(
    context_window=4096,
    max_new_tokens=256,
    generate_kwargs={"temperature": 0.0, "do_sample": False},
    system_prompt=system_prompt,
    query_wrapper_prompt=query_wrapper_prompt,
    tokenizer_name="meta-llama/llama-2-7b-chat-hf",
    model_name="meta-llama/llama-2-7b-chat-hf",
    device_map="auto",  # Uses GPU if available
    model_kwargs={"torch_dtype": torch.float16 if device == "cuda" else torch.float32}
)

# Embeddings with LangChain-compatible wrapper
embed_model = LangchainEmbedding(
    HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")
)

# Settings
Settings.embed_model = embed_model
Settings.llm = llm
Settings.chunk_size = 24

# ==== Main Function ====
def chat_with_file(uploaded_file, question):
    try:
        input_dir = "uploaded_data"
        os.makedirs(input_dir, exist_ok=True)

        # Save uploaded file
        file_path = os.path.join(input_dir, uploaded_file.name)
        with open(file_path, "wb") as f:
            f.write(uploaded_file.read())

        # Load, Index, and Query
        documents = SimpleDirectoryReader(input_dir=input_dir).load_data()
        index = VectorStoreIndex.from_documents(documents)
        query_engine = index.as_query_engine()
        response = query_engine.query(question)

        # Clean up
        shutil.rmtree(input_dir)

        return str(response)

    except Exception as e:
        return f"‚ùå Error: {e}"

# ==== Gradio Interface ====
iface = gr.Interface(
    fn=chat_with_file,
    inputs=[
        gr.File(label="Upload a PDF or TXT file"),
        gr.Textbox(label="Ask your question")
    ],
    outputs=gr.Textbox(label="Answer"),
    title="üìÑüí¨ LLaMA2 RAG Chatbot (Colab + GPU)",
    description="Upload a document and ask questions. Powered by LLaMA2, embeddings, and GPU acceleration."
)

iface.launch(share=True)